import numpy as np, tensorflow as tf
from stable_baselines.common.replay_buffer import ReplayBuffer
from stable_baselines.common import tf_util
from abc import ABC, abstractmethod
import faiss

class AbstractLandmarkGenerator(ABC):
  """
  Defines interface for landmark generators
  """

  def __init__(self, buffer_size, env):
    self.buffer_size = buffer_size
    self.ac_space = env.action_space
    self.ob_space = env.observation_space.spaces['observation']
    self.goal_space = env.observation_space.spaces['achieved_goal']
    if not env.goal_extraction_function:
      raise ValueError("Environment must have goal_extraction_function method")
    self.goal_extraction_function = env.goal_extraction_function

    self.states = None
    self.landmark_states = None
    self.goals = None

    self.use_actions = False
    self.actions = None

  @abstractmethod
  def add_state_data(self, states, achieved_goals):
    """
    Processes state / achieved_goal data.
    """
    raise NotImplementedError

  @abstractmethod
  def add_landmark_experience_data(self, states, actions, landmarks, desired_goals, additional):
    """
    Processes state / action / landmark / goal data.
    """
    raise NotImplementedError

  @abstractmethod
  def generate(self, states, actions, goals):
    """Generates landmarks for states and goals (and optionally, actions).

    :param states: (batch_size * state_dim vector)
    :param states: (batch_size * action_dim vector)
    :param goals: (batch_size * goal_dim vector)
    :return landmarks: (tuple of (landmark_states, landmark_goals))
    """
    raise NotImplementedError

  @abstractmethod
  def assign_scores(self, scores, ratios):
    """Assigns scores to the landmarks previously generated by generate method."""
    raise NotImplementedError

class RandomLandmarkGenerator(AbstractLandmarkGenerator):
  def __init__(self, buffer_size, env):
    super().__init__(buffer_size, env)
    self.state_buffer = ReplayBuffer(self.buffer_size, [("state", self.ob_space.shape)])
    if self.goal_extraction_function is None:
      raise ValueError("Random generator requires a goal_extraction function!")
  
  def add_state_data(self, states, goals):
    self.state_buffer.add_batch(states)

  def add_landmark_experience_data(self, states, actions, landmarks, desired_goals, additional=None):
    # Does nothing with landmark experiences
    pass

  def generate(self, states, actions, goals):

    # Does not use actions
    self.landmark_states = landmark_states = self.state_buffer.sample(len(states))[0]
    landmark_goals = self.goal_extraction_function(landmark_states)
    self.states = states
    self.actions = actions
    self.goals = goals

    return landmark_states, landmark_goals

  def assign_scores(self, scores, ratios):
    # Do Nothing (random generator does not learn)
    pass

  def __len__(self):
    return len(self.state_buffer)

class NearestNeighborLandmarkGenerator(AbstractLandmarkGenerator):
  def __init__(self, buffer_size, env, epsilon=0.3, time_scale=0.000001, score_cutoff=0.96, threshold_nn_size=1000, max_size=200000):
    super().__init__(buffer_size, env)
    self.state_buffer = ReplayBuffer(self.buffer_size, [("state", self.ob_space.shape)])
    
    self.state_buffer = ReplayBuffer(self.buffer_size, [("state", self.ob_space.shape)])
    self.landmark_buffer = ReplayBuffer(self.buffer_size, [("state", self.ob_space.shape),
                      ("action", self.ac_space.shape),
                      ("landmark", self.ob_space.shape),
                      ("desired_goal", self.goal_space.shape)
                ])
    
    self.d = self.ob_space.shape[-1] + self.goal_space.shape[-1] + 1
    self.index = faiss.IndexFlatL2(self.d)
    self.score_cutoff = score_cutoff
    self.time_scale = time_scale
    self.epsilon = epsilon
    self.threshold_nn_size = threshold_nn_size
    self.landmarks = np.zeros((0, self.ob_space.shape[-1]))
    self.time = 0.
    self.max_size = max_size
    if self.goal_extraction_function is None:
      raise ValueError("Random generator requires a goal_extraction function!")

  def add_state_data(self, states, goals):
    self.state_buffer.add_batch(states)

  def add_landmark_experience_data(self, states, actions, landmarks, desired_goals, additional=None):
    self.landmark_buffer.add_batch(states, actions, landmarks, desired_goals)

  def generate(self, states, actions, goals):
    self.states = states
    self.actions = actions
    self.goals = goals
    nn_size = self.index.ntotal

    if nn_size > self.max_size:
      #prune half of the nn database
      self.index.remove_ids(faiss.IDSelectorRange(0, self.max_size//2))
      self.landmarks = self.landmarks[self.max_size//2:]
      nn_size = self.index.ntotal
      assert(len(self.landmarks) == nn_size)

    if nn_size < self.threshold_nn_size:
      self.landmark_states = landmark_states = self.state_buffer.sample(len(states))[0]
    else:
      num_random = int(self.epsilon * len(states))
      random_landmarks = self.state_buffer.sample(num_random)[0]
      
      time_feature = np.ones((len(states)-num_random,1)) * self.time
      query = np.concatenate([states[num_random:], goals[num_random:], time_feature], 1).astype('float32')
      _, lidxs = self.index.search(query, 1)
      genned_landmarks = self.landmarks[lidxs[:,0]]

      self.landmark_states = landmark_states = np.concatenate((random_landmarks, genned_landmarks), 0)
    
    landmark_goals = self.goal_extraction_function(landmark_states)
    return landmark_states, landmark_goals

  def assign_scores(self, scores, ratios):
    # Do Nothing (random generator does not learn)
    self.time += self.time_scale
    time_feature = np.ones((len(self.states),1)) * self.time
    
    values = np.concatenate((self.states, self.goals, time_feature), 1)

    saved_idxs = np.squeeze(np.argwhere(scores > self.score_cutoff),1)
    saved_values = values[saved_idxs]
    saved_landmarks = self.landmark_states[saved_idxs]

    self.index.add(saved_values.astype('float32'))
    self.landmarks = np.concatenate((self.landmarks, saved_landmarks), 0)

  def __len__(self):
    return self.index.ntotal



class NonScoreBasedVAEWithNNRefinement(AbstractLandmarkGenerator):
  def __init__(self, buffer_size, env, refine_with_NN=False, use_actions=True, batch_size=128, num_training_steps=100, learning_threshold=100, max_nn_index_size=200000):
    super().__init__(buffer_size, env)

    self.refine_with_NN = refine_with_NN
    self.use_actions = use_actions
    self.hidden_dim = 400
    self.z_dim = 50
    self.batch_size = batch_size
    self.num_training_steps = num_training_steps
    self.learning_threshold = learning_threshold
    self.max_nn_index_size = max_nn_index_size
    if self.goal_extraction_function is None:
      raise ValueError("NonScoreBasedVAEWithNNRefinement requires a goal_extraction function!")
    self.steps = 0

    ## Replay Buffers ##
    self.landmarks = np.zeros((0, self.ob_space.shape[-1]))
    self.landmark_buffer = ReplayBuffer(self.buffer_size, [("state", self.ob_space.shape),
                      ("action", self.ac_space.shape),
                      ("landmark", self.ob_space.shape),
                      ("desired_goal", self.goal_space.shape)
                ])

    ## NN Index ##
    self.d = self.ob_space.shape[-1]
    self.index = faiss.IndexFlatL2(self.d)

    ## CVAE Graph ##
    self.graph = tf.Graph()
    with self.graph.as_default():
      self.sess = tf_util.make_session(graph=self.graph)
      
      sag_len = self.ob_space.shape[-1] + self.goal_space.shape[-1]
      if self.use_actions:
        sag_len += self.ac_space.shape[-1]
      state_action_goal = tf.placeholder(tf.float32, [None, sag_len], name='sag_placeholder')
      landmark = tf.placeholder(tf.float32, [None, self.ob_space.shape[-1]], name='lm_placeholder')

      # encoder

      h = tf.layers.dense(tf.concat([state_action_goal, landmark], axis=1), self.hidden_dim, activation=tf.nn.relu)
      h = tf.layers.dense(h, self.hidden_dim, activation=tf.nn.relu)
      mu = tf.layers.dense(h, self.z_dim)
      log_variance = tf.layers.dense(h, self.z_dim)
      z = tf.random_normal(shape=tf.shape(mu)) * tf.sqrt(tf.exp(log_variance)) + mu

      # decoder

      h = tf.layers.dense(tf.concat([state_action_goal, z], axis=1), self.hidden_dim, activation=tf.nn.relu)
      h = tf.layers.dense(h, self.hidden_dim, activation=tf.nn.relu)
      generated_landmark = tf.layers.dense(h, self.ob_space.shape[-1])

      # Distortion is the negative log likelihood:  P(X|z,c)
      l2_loss = tf.reduce_sum(tf.squared_difference(landmark, generated_landmark), 1)
      tf.summary.scalar("VAE_distortion_l2Loss", tf.reduce_mean(l2_loss))

      # The rate is the D_KL(Q(z|X,y)||P(z|c))
      latent_loss = -0.5*tf.reduce_sum(1.0 + log_variance - tf.square(mu) - tf.exp(log_variance), 1)
      tf.summary.scalar("VAE_rate_LatentLoss", tf.reduce_mean(latent_loss))
      loss = tf.reduce_mean(l2_loss + latent_loss)
      tf.summary.scalar("VAE_elbo", loss)
      opt = tf.train.AdamOptimizer()

      gradients = opt.compute_gradients(loss, var_list=tf.trainable_variables())
      for i, (grad, var) in enumerate(gradients):
        if grad is not None:
          gradients[i] = (tf.clip_by_norm(grad, 1.), var)

      ts = opt.apply_gradients(gradients)
      init = tf.global_variables_initializer()

      # TODO: Log the generated outputs
      # tf.summary.image("VAE_gen_landmark", generated_landmark, max_outputs=1)
      # tf.summary.image("VAE_train_landmark", landmark, max_outputs=1)

      self.summary = tf.summary.merge_all()

    self.sess.run(init)

    self.g = {
      'sag_ph': state_action_goal,
      'lm_ph': landmark,
      'z': z,
      'generated_landmark': generated_landmark,
      'loss': loss,
      'ts': ts,
      'summary': self.summary
    }

  def add_state_data(self, states, goals):
    """Add state data to buffer (for initial random generation), and also to NN Index"""
    self.landmarks = np.concatenate((self.landmarks, states), 0)
    self.index.add(states.astype('float32'))    
    
    nn_size = self.index.ntotal

    if nn_size > self.max_nn_index_size:
      #prune half of the nn database
      #I verified that this works as intended: when we remove ids from faiss, all ids get bumped up.
      self.index.remove_ids(faiss.IDSelectorRange(0, self.max_nn_index_size//2))
      self.landmarks = self.landmarks[self.max_nn_index_size//2:]
      nn_size = self.index.ntotal
      assert(len(self.landmarks) == nn_size)

  def add_landmark_experience_data(self, states, actions, landmarks, desired_goals, additional=None):

    self.landmark_buffer.add_batch(states, actions, landmarks, desired_goals)
    
    loss = 0
    # run some training steps
    for _ in range(self.num_training_steps):
      self.steps +=1
      s, a, l, g = self.landmark_buffer.sample(self.batch_size)
      
      if self.use_actions:
        feed_dict = {self.g['sag_ph']: np.concatenate((s, a, g), axis=1),
        self.g['lm_ph']: l}
      else:
        feed_dict = {self.g['sag_ph']: np.concatenate((s, g), axis=1),
        self.g['lm_ph']: l}

      _, loss_, summary = self.sess.run([self.g['ts'], self.g['loss'], self.g['summary']], feed_dict=feed_dict)
      loss += loss_

    if self.steps % 100 == 0:
      print("Landmark CVAE step {}: loss {}".format(self.steps, loss / self.num_training_steps))

    return summary

  def generate(self, states, actions, goals):
    self.states = states
    self.actions = actions
    self.goals = goals

    # Generate randomly at first
    if self.steps < self.learning_threshold:
      self.landmark_states = landmark_states = np.random.choice(self.landmarks, size=len(states))
      landmark_goals = self.goal_extraction_function(landmark_states)

      return landmark_states, landmark_goals

    # Otherwise, generate using the VAE

    sampled_zs = np.random.normal(size=(len(states), self.z_dim))
    if self.use_actions:
      feed_dict = {self.g['z']: sampled_zs, self.g['sag_ph']: np.concatenate([states, actions, goals], axis=1)}
    else:
      feed_dict = {self.g['z']: sampled_zs, self.g['sag_ph']: np.concatenate([states, goals], axis=1)}
    landmark_states = self.sess.run(self.g['generated_landmark'], feed_dict=feed_dict)

    if self.refine_with_NN:
      query = landmark_states.astype('float32')
      _, lidxs = self.index.search(query, 1)
      landmark_states = self.landmarks[lidxs[:,0]]

    landmark_goals = self.goal_extraction_function(landmark_states)
    return landmark_states, landmark_goals

  def assign_scores(self, scores, ratios):
    # Do Nothing (this is not a score-based generator)
    pass

  def __len__(self):
    return self.index.ntotal



class ScoreBasedVAEWithNNRefinement(AbstractLandmarkGenerator):
  def __init__(self, buffer_size, env, refine_with_NN=False, use_actions=True, batch_size=128, num_training_steps=100, learning_threshold=100, max_nn_index_size=200000):
    super().__init__(buffer_size, env)

    self.get_scores_with_experiences = True

    self.refine_with_NN = refine_with_NN
    self.use_actions = use_actions
    self.hidden_dim = 400
    self.z_dim = 50
    self.batch_size = batch_size
    self.num_training_steps = num_training_steps
    self.learning_threshold = learning_threshold
    self.max_nn_index_size = max_nn_index_size
    if self.goal_extraction_function is None:
      raise ValueError("NonScoreBasedVAEWithNNRefinement requires a goal_extraction function!")
    self.steps = 0

    ## Replay Buffers ##
    self.landmarks = np.zeros((0, self.ob_space.shape[-1]))
    self.landmark_buffer = ReplayBuffer(self.buffer_size, [("state", self.ob_space.shape),
                      ("action", self.ac_space.shape),
                      ("landmark", self.ob_space.shape),
                      ("desired_goal", self.goal_space.shape),
                      ("scores", (2,)) # score & ratio
                ])

    ## NN Index ##
    self.d = self.ob_space.shape[-1]
    self.index = faiss.IndexFlatL2(self.d)

    ## CVAE Graph ##
    self.graph = tf.Graph()
    with self.graph.as_default():
      self.sess = tf_util.make_session(graph=self.graph)
      
      sag_len = self.ob_space.shape[-1] + self.goal_space.shape[-1] + 2
      if self.use_actions:
        sag_len += self.ac_space.shape[-1]
      state_action_goal_scores = tf.placeholder(tf.float32, [None, sag_len], name='sag_placeholder')
      landmark = tf.placeholder(tf.float32, [None, self.ob_space.shape[-1]], name='lm_placeholder')

      # encoder

      h = tf.layers.dense(tf.concat([state_action_goal_scores, landmark], axis=1), self.hidden_dim, activation=tf.nn.relu)
      h = tf.layers.dense(h, self.hidden_dim, activation=tf.nn.relu)
      mu = tf.layers.dense(h, self.z_dim)
      log_variance = tf.layers.dense(h, self.z_dim)
      z = tf.random_normal(shape=tf.shape(mu)) * tf.sqrt(tf.exp(log_variance)) + mu

      # decoder

      h = tf.layers.dense(tf.concat([state_action_goal_scores, z], axis=1), self.hidden_dim, activation=tf.nn.relu)
      h = tf.layers.dense(h, self.hidden_dim, activation=tf.nn.relu)
      generated_landmark = tf.layers.dense(h, self.ob_space.shape[-1])


      l2_loss = tf.reduce_sum(tf.squared_difference(landmark, generated_landmark), 1)
      tf.summary.scalar("VAE_distortion_l2Loss", l2_loss)
      latent_loss = -0.5*tf.reduce_sum(1.0 + log_variance - tf.square(mu) - tf.exp(log_variance), 1)
      tf.summary.scalar("VAE_rate_LatentLoss", latent_loss)
      loss = tf.reduce_mean(l2_loss + latent_loss)
      tf.summary.scalar("VAE_elbo", loss)
      opt = tf.train.AdamOptimizer()

      gradients = opt.compute_gradients(loss, var_list=tf.trainable_variables())
      for i, (grad, var) in enumerate(gradients):
        if grad is not None:
          gradients[i] = (tf.clip_by_norm(grad, 1.), var)

      ts = opt.apply_gradients(gradients)

      init = tf.global_variables_initializer()

      self.summary = tf.summary.merge_all()

    self.sess.run(init)
    self.g = {
      'sagadd_ph': state_action_goal_scores,
      'lm_ph': landmark,
      'z': z,
      'generated_landmark': generated_landmark,
      'loss': loss,
      'ts': ts,
      'summary': self.summary
    }
    }

  def add_state_data(self, states, goals):
    """Add state data to buffer (for initial random generation), and also to NN Index"""
    self.landmarks = np.concatenate((self.landmarks, states), 0)
    self.index.add(states.astype('float32'))    
    
    nn_size = self.index.ntotal

    if nn_size > self.max_nn_index_size:
      #prune half of the nn database
      #I verified that this works as intended: when we remove ids from faiss, all ids get bumped up.
      self.index.remove_ids(faiss.IDSelectorRange(0, self.max_nn_index_size//2))
      self.landmarks = self.landmarks[self.max_nn_index_size//2:]
      nn_size = self.index.ntotal
      assert(len(self.landmarks) == nn_size)

  def add_landmark_experience_data(self, states, actions, landmarks, desired_goals, additional):

    self.landmark_buffer.add_batch(states, actions, landmarks, desired_goals, additional)
    
    loss = 0
    # run some training steps
    for _ in range(self.num_training_steps):
      self.steps +=1
      s, a, l, g, add = self.landmark_buffer.sample(self.batch_size)

      if self.use_actions:
        feed_dict = {self.g['sagadd_ph']: np.concatenate((s, a, g, add), axis=1),
        self.g['lm_ph']: l}
      else:
        feed_dict = {self.g['sagadd_ph']: np.concatenate((s, g, add), axis=1),
        self.g['lm_ph']: l}

      _, loss_, summary = self.sess.run([self.g['ts'], self.g['loss'], self.g['summary']], feed_dict=feed_dict)
      loss += loss_

    if self.steps % 100 == 0:
      print("Landmark CVAE step {}: loss {}".format(self.steps, loss / self.num_training_steps))

    return summary

  def generate(self, states, actions, goals):
    self.states = states
    self.actions = actions
    self.goals = goals

    # Generate randomly at first
    if self.steps < self.learning_threshold:
      self.landmark_states = landmark_states = np.random.choice(self.landmarks, size=len(states))
      landmark_goals = self.goal_extraction_function(landmark_states)

      return landmark_states, landmark_goals

    # Otherwise, generate using the VAE

    sampled_zs = np.random.normal(size=(len(states), self.z_dim))
    scores = np.concatenate([np.ones((len(states), 1)), np.zeros((len(states), 1))], 1) # condition on score = 1, ratio = 0.

    if self.use_actions:
      feed_dict = {self.g['z']: sampled_zs, self.g['sagadd_ph']: np.concatenate([states, actions, goals, scores], axis=1)}
    else:
      feed_dict = {self.g['z']: sampled_zs, self.g['sagadd_ph']: np.concatenate([states, goals, scores], axis=1)}
    landmark_states = self.sess.run(self.g['generated_landmark'], feed_dict=feed_dict)

    if self.refine_with_NN:
      query = landmark_states.astype('float32')
      _, lidxs = self.index.search(query, 1)
      landmark_states = self.landmarks[lidxs[:,0]]

    landmark_goals = self.goal_extraction_function(landmark_states)
    return landmark_states, landmark_goals

  def assign_scores(self, scores, ratios):
    # Do Nothing (this is not a score-based generator)
    pass

  def __len__(self):
    return self.index.ntotal

